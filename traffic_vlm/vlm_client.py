from __future__ import annotations

import base64
import json
import os
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple

from openai import OpenAI, AsyncOpenAI
import asyncio

from .config import VLMConfig


# [Windowsç¼–ç ä¿®å¤] å®‰å…¨çš„printå‡½æ•°ï¼Œé¿å…åœ¨éUTF-8æ§åˆ¶å°æ—¶è§¦å‘Errno 22
def _safe_print(*args, **kwargs):
    """å®‰å…¨çš„printåŒ…è£…ï¼Œå¤„ç†Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜"""
    try:
        print(*args, **kwargs)
    except OSError:
        kwargs.pop('flush', None)
        try:
            safe_args = []
            for arg in args:
                if isinstance(arg, str):
                    safe_args.append(arg.encode('ascii', errors='replace').decode('ascii'))
                else:
                    safe_args.append(arg)
            print(*safe_args, **kwargs)
        except Exception:
            pass


def _extract_json_from_markdown(text: str) -> str:
    """
    ä»markdownä»£ç å—ä¸­æå–JSONå†…å®¹

    VLMæœ‰æ—¶ä¼šè¿”å›è¢«markdownä»£ç å—åŒ…è£¹çš„JSONï¼š
    ```json
    {"key": "value"}
    ```

    æ­¤å‡½æ•°å»é™¤åŒ…è£¹ï¼Œè¿”å›çº¯JSONå­—ç¬¦ä¸²
    """
    if not text:
        return text

    text = text.strip()

    # åŒ¹é… ```json ... ``` æˆ– ``` ... ```
    pattern = r'```(?:json)?\s*\n?(.*?)\n?```'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()

    return text


def _save_vlm_request_log(
    mode: str,
    model: str,
    temperature: float,
    system_prompt: str,
    user_prompt: str,
    image_paths: List[str],
    response_text: str,
    parsed_response: Dict,
    clip_info: Optional[Dict] = None,
    base_dir: str = "data",
) -> str:
    """ä¿å­˜VLMè¯·æ±‚å’Œå“åº”æ•°æ®åˆ°æ–‡ä»¶"""
    log_dir = os.path.join(base_dir, "vlm_logs")
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    log_file = os.path.join(log_dir, f"vlm_request_{mode}_{timestamp}.json")

    log_data = {
        "timestamp": datetime.now().isoformat(),
        "mode": mode,
        "model": model,
        "temperature": temperature,
        # Clipä¿¡æ¯ï¼ˆæ¥è‡ªpipelineçš„å€™é€‰ç‰‡æ®µï¼‰
        "clip_info": clip_info or {},
        "system_prompt": system_prompt,
        "user_prompt": user_prompt,
        "image_paths": image_paths,
        "image_count": len(image_paths),
        "response_raw": response_text,
        "response_parsed": parsed_response,
    }

    with open(log_file, "w", encoding="utf-8") as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)

    _safe_print(f"[VLMæ—¥å¿—] å·²ä¿å­˜åˆ°: {log_file}")
    return log_file


SYSTEM_PROMPT = """
ä½ æ˜¯ä¸“ä¸šçš„äº¤é€šè¿æ³•ä¸äº‹æ•…åˆ†æä¸“å®¶ï¼Œå…·å¤‡ä¸°å¯Œçš„é“è·¯äº¤é€šå®‰å…¨åˆ†æç»éªŒã€‚

ä½ ä¼šæ”¶åˆ°ï¼š
1. è·¯å£ç›‘æ§å…³é”®å¸§å›¾ç‰‡ï¼ˆå·²æ ‡æ³¨è½¦é“åŒºåŸŸã€ç›®æ ‡IDå’Œè½¨è¿¹ï¼‰
2. è¯¦ç»†çš„è·¯å£é…ç½®å’Œç›®æ ‡è½¨è¿¹ä¿¡æ¯

ä½ çš„ä»»åŠ¡æ˜¯å…¨é¢åˆ†æå¹¶è¯†åˆ«ä»¥ä¸‹äº¤é€šäº‹ä»¶ï¼š

ã€äºŒè½®è½¦è¿æ³•è¡Œä¸ºã€‘
- bike_wrong_way: äºŒè½®è½¦é€†è¡Œï¼ˆç”µåŠ¨è½¦ã€è‡ªè¡Œè½¦ã€æ‘©æ‰˜è½¦åœ¨æœºåŠ¨è½¦é“æˆ–éæœºåŠ¨è½¦é“é€†è¡Œï¼‰
- run_red_light_bike: äºŒè½®è½¦é—¯çº¢ç¯ï¼ˆä¿¡å·ç¯ä¸ºçº¢è‰²æ—¶ç»§ç»­é€šè¿‡è·¯å£ï¼‰
- occupy_motor_lane_bike: äºŒè½®è½¦å ç”¨æœºåŠ¨è½¦é“ï¼ˆéæœºåŠ¨è½¦è¿›å…¥æœºåŠ¨è½¦é“è¡Œé©¶ï¼‰
- bike_improper_turning: äºŒè½®è½¦è¿è§„è½¬å¼¯ï¼ˆæœªæŒ‰è§„å®šè½¦é“è½¬å¼¯ã€éšæ„å˜é“ï¼‰
- bike_illegal_u_turn: äºŒè½®è½¦è¿è§„æ‰å¤´ï¼ˆç¦æ­¢æ‰å¤´å¤„æ‰å¤´ã€å½±å“æ­£å¸¸è¡Œé©¶ï¼‰

ã€æœºåŠ¨è½¦è¿æ³•è¡Œä¸ºã€‘
- car_wrong_way: æœºåŠ¨è½¦é€†è¡Œï¼ˆåœ¨ç¦æ­¢é€†è¡Œè·¯æ®µé€†å‘è¡Œé©¶ï¼‰
- run_red_light_car: æœºåŠ¨è½¦é—¯çº¢ç¯ï¼ˆä¿¡å·ç¯ä¸ºçº¢è‰²æ—¶ç»§ç»­é€šè¿‡è·¯å£ï¼‰
- illegal_parking: è¿æ³•åœè½¦ï¼ˆåœ¨ç¦åœåŒºåŸŸã€å½±å“äº¤é€šçš„ä½ç½®åœè½¦ï¼‰
- illegal_u_turn: æœºåŠ¨è½¦è¿è§„æ‰å¤´ï¼ˆç¦æ­¢æ‰å¤´å¤„æ‰å¤´ã€å½±å“æ­£å¸¸è¡Œé©¶ï¼‰
- speeding: è¶…é€Ÿè¡Œé©¶ï¼ˆè¶…è¿‡é™é€Ÿæ ‡å¿—æ˜¾ç¤ºçš„é€Ÿåº¦ï¼‰
- illegal_overtaking: è¿è§„è¶…è½¦ï¼ˆåœ¨ç¦æ­¢è¶…è½¦åŒºåŸŸæˆ–æ¡ä»¶ä¸å…è®¸æ—¶è¶…è½¦ï¼‰
- improper_lane_change: è¿è§„å˜é“ï¼ˆæœªæ‰“è½¬å‘ç¯ã€å½±å“å…¶ä»–è½¦è¾†æ­£å¸¸è¡Œé©¶ï¼‰

ã€äº¤é€šäº‹æ•…ã€‘
- vehicle_to_vehicle_accident: æœºåŠ¨è½¦ä¹‹é—´äº‹æ•…ï¼ˆä¸¤è¾†æˆ–å¤šè¾†æœºåŠ¨è½¦å‘ç”Ÿç¢°æ’ï¼‰
- vehicle_to_bike_accident: æœºåŠ¨è½¦ä¸äºŒè½®è½¦äº‹æ•…ï¼ˆæœºåŠ¨è½¦ä¸ç”µåŠ¨è½¦ã€è‡ªè¡Œè½¦ã€æ‘©æ‰˜è½¦ç¢°æ’ï¼‰
- vehicle_to_pedestrian_accident: æœºåŠ¨è½¦ä¸è¡Œäººäº‹æ•…ï¼ˆæœºåŠ¨è½¦ä¸è¡Œäººå‘ç”Ÿç¢°æ’ï¼‰
- multi_vehicle_accident: å¤šè½¦è¿æ’äº‹æ•…ï¼ˆä¸‰è¾†æˆ–ä»¥ä¸Šè½¦è¾†è¿ç¯ç›¸æ’ï¼‰
- hit_and_run: è‚‡äº‹é€ƒé€¸ï¼ˆå‘ç”Ÿäº‹æ•…åé€ƒç¦»ç°åœºï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦è¾“å‡ºä»»ä½•å¤šä½™å†…å®¹ï¼š
{
  "has_violation": bool,
  "violations": [
    {
      "type": "è¿æ³•æˆ–äº‹æ•…ç±»å‹",
      "confidence": ç½®ä¿¡åº¦(0.0-1.0),
      "tracks": [æ¶‰åŠçš„ç›®æ ‡IDåˆ—è¡¨],
      "start_time": "å¼€å§‹æ—¶é—´ ISOæ ¼å¼",
      "end_time": "ç»“æŸæ—¶é—´ ISOæ ¼å¼",
      "evidence": "åˆ¤æ–­ä¾æ®çš„ç®€è¦è¯´æ˜",
      "behavior_before": "äº‹ä»¶å‘ç”Ÿå‰çš„ç›®æ ‡è¡Œä¸ºæè¿°",
      "behavior_after": "äº‹ä»¶å‘ç”Ÿåçš„ç›®æ ‡è¡Œä¸ºæè¿°",
      "trajectory_description": "è¯¦ç»†çš„è½¨è¿¹æè¿°ï¼ˆè¡Œé©¶è·¯å¾„ã€é€Ÿåº¦å˜åŒ–ã€æ–¹å‘å˜åŒ–ç­‰ï¼‰",
      "weather_condition": "å¤©æ°”æƒ…å†µï¼ˆå¦‚ï¼šæ™´ã€é˜´ã€é›¨ã€é›ªã€é›¾ç­‰ï¼‰",
      "road_condition": "è·¯é¢æƒ…å†µï¼ˆå¦‚ï¼šå¹²ç‡¥ã€æ¹¿æ»‘ã€ç§¯æ°´ã€ç»“å†°ç­‰ï¼‰",
      "traffic_light_status": "ä¿¡å·ç¯çŠ¶æ€ï¼ˆå¦‚æœ‰ï¼‰",
      "other_details": "å…¶ä»–ç›¸å…³æè¿°ï¼ˆè½¦é“å ç”¨æƒ…å†µã€å‘¨è¾¹ç¯å¢ƒã€ç‰¹æ®Šæƒ…å½¢ç­‰ï¼‰"
    }
  ],
  "text_summary": "å¯¹æ•´ä¸ªç‰‡æ®µçš„è¯¦ç»†è‡ªç„¶è¯­è¨€æè¿°ï¼ŒåŒ…å«äº‹ä»¶å…¨è¿‡ç¨‹çš„å®Œæ•´ä¿¡æ¯"
}
"""

# äº¤é€šäº‹æ•…æ£€ç´¢æ¨¡å¼ä¸“ç”¨ System Promptï¼ˆå››æ€è¾“å‡ºï¼šYES/POST_EVENT_ONLY/UNCERTAIN/NOï¼‰
ACCIDENT_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸“ä¸šçš„äº¤é€šäº‹æ•…é‰´å®šä¸“å®¶ï¼Œè´Ÿè´£ç²¾å‡†è¯†åˆ«é“è·¯äº¤é€šäº‹æ•…ã€‚

**æ ¸å¿ƒåŸåˆ™ï¼šè¯æ®é©±åŠ¨ï¼ŒåŒºåˆ†ç¢°æ’è¿‡ç¨‹ä¸åæœã€‚**

ä½ ä¼šæ”¶åˆ°ï¼š
1. è·¯å£ç›‘æ§å…³é”®å¸§å›¾ç‰‡ï¼ˆåŒ…å«åŸå›¾ã€ROIè£å‰ªã€æ ‡æ³¨å›¾ï¼‰
2. è¯¦ç»†çš„è·¯å£é…ç½®å’Œç›®æ ‡è½¨è¿¹ä¿¡æ¯

ã€å››æ€åˆ¤å®šè§„åˆ™ã€‘â­â­â­

**verdict = "YES"**ï¼ˆç¡®è®¤äº‹æ•… - çœ‹åˆ°ç¢°æ’è¿‡ç¨‹ï¼‰
  å¿…é¡»æ»¡è¶³ä»¥ä¸‹è‡³å°‘ä¸€é¡¹ç¡¬æ€§è¯æ®ï¼š
  - å¯è§ä¸¤ä¸ªç›®æ ‡å‘ç”Ÿæ˜æ˜¾ç‰©ç†æ¥è§¦/ç¢°æ’çš„ç¬é—´
  - ç¢°æ’ç¬é—´æœ‰å½¢å˜ã€æ•£è½ç¢ç‰‡
  - äººå‘˜è¢«æ’å€’åœ°çš„è¿‡ç¨‹
  - è½¦è¾†ç¢°æ’å¯¼è‡´æŸåå˜å½¢
  - æ³¨ï¼šè½»å¾®å‰è¹­ä¹Ÿç®—äº‹æ•…ï¼Œåªè¦èƒ½çœ‹åˆ°æ¥è§¦è¿‡ç¨‹

**verdict = "POST_EVENT_ONLY"**ï¼ˆä»…åæœ - æœªçœ‹åˆ°ç¢°æ’ä½†æœ‰æ˜ç¡®åæœï¼‰â­æ–°å¢
  é€‚ç”¨åœºæ™¯ï¼š
  - æœªçœ‹åˆ°ç¢°æ’/æ¥è§¦çš„ç¬é—´
  - ä½†çœ‹åˆ°æ˜ç¡®çš„äº‹æ•…åæœï¼š
    Â· äººå‘˜å€’åœ°ä¸èµ·ï¼ˆèººåœ¨åœ°é¢ï¼‰
    Â· è½¦è¾†æ˜æ˜¾å—æŸ/å˜å½¢
    Â· è·¯é¢æœ‰æ•£è½ç¢ç‰‡/æ¶²ä½“æ³„æ¼
    Â· å…¶ä»–è½¦è¾†æ˜æ˜¾ç»•è¡Œé¿è®©
    Â· äº¤è­¦/æ•‘æ´äººå‘˜æ­£åœ¨ç°åœºå¤„ç†
    Â· ä¸‰è§’è­¦ç¤ºç‰Œã€è­¦ç¤ºé”¥æ¡¶
  - è¿™æ˜¯"äº‹æ•…åæœç‰‡æ®µ"ï¼Œéœ€è¦è¡¥å……æ›´æ—©çš„ç”»é¢æ‰èƒ½ç¡®è®¤å®Œæ•´è¿‡ç¨‹

**verdict = "UNCERTAIN"**ï¼ˆä¸ç¡®å®šï¼Œéœ€äººå·¥å¤æ ¸ï¼‰
  ä»¥ä¸‹æƒ…å†µå¿…é¡»é€‰æ‹©UNCERTAINï¼š
  - ç”»é¢æ¨¡ç³Š/é®æŒ¡æ— æ³•çœ‹æ¸…å…³é”®ç¬é—´
  - çœ‹èµ·æ¥æœ‰å±é™©ä½†æ— æ³•ç¡®è®¤æ˜¯å¦å‘ç”Ÿæ¥è§¦
  - ç›®æ ‡ä½ç½®éå¸¸æ¥è¿‘ä½†ç¢°æ’ç¬é—´ä¸åœ¨ç”»é¢ä¸­
  - æœ‰å¼‚å¸¸åœç•™ä½†åŸå› ä¸æ˜
  - è¯æ®ä¸å……åˆ†ï¼Œæ— æ³•åšå‡ºåˆ¤æ–­

**verdict = "NO"**ï¼ˆç¡®è®¤æ— äº‹æ•…ï¼‰
  å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š
  - æ²¡æœ‰çœ‹åˆ°ä»»ä½•ç¢°æ’/æ¥è§¦è¯æ®
  - æ²¡æœ‰çœ‹åˆ°ä»»ä½•äº‹æ•…åæœï¼ˆæ— å€’åœ°ã€æ— æŸåã€æ— ç¢ç‰‡ï¼‰
  - æ‰€æœ‰ç›®æ ‡è¡Œä¸ºæ­£å¸¸ï¼ˆæ­£å¸¸è¡Œé©¶ã€æ­£å¸¸ç­‰ç¯ï¼‰
  - åœºæ™¯æ˜ç¡®æ˜¯æ­£å¸¸äº¤é€šæµ

âš ï¸ **é‡è¦**ï¼š
- çœ‹åˆ°åæœä½†æœªçœ‹åˆ°ç¢°æ’è¿‡ç¨‹ â†’ POST_EVENT_ONLYï¼ˆä¸æ˜¯NOï¼‰
- ä¸ç¡®å®šæ—¶ â†’ UNCERTAINï¼ˆä¸æ˜¯NOï¼‰
- åªæœ‰æ˜ç¡®æ— äº‹æ•…ä¸”æ— åæœæ—¶æ‰é€‰ NO

ã€äº¤é€šäº‹æ•…ç±»å‹ã€‘
- vehicle_to_vehicle_accident: æœºåŠ¨è½¦ä¹‹é—´ç¢°æ’
- vehicle_to_bike_accident: æœºåŠ¨è½¦ä¸äºŒè½®è½¦ç¢°æ’
- vehicle_to_pedestrian_accident: æœºåŠ¨è½¦ä¸è¡Œäººç¢°æ’
- multi_vehicle_accident: å¤šè½¦è¿æ’
- hit_and_run: è‚‡äº‹é€ƒé€¸

ã€å‚ä¸è€…ç±»å‹ã€‘
- car-car: æœºåŠ¨è½¦ä¹‹é—´
- car-bike: æœºåŠ¨è½¦ä¸äºŒè½®è½¦
- car-ped: æœºåŠ¨è½¦ä¸è¡Œäºº
- multi: å¤šæ–¹å‚ä¸
- other: å…¶ä»–

ã€æ˜ç¡®æ’é™¤çš„æ­£å¸¸åœºæ™¯ã€‘
âŒ æ­£å¸¸ç­‰çº¢ç¯çš„æ’é˜Ÿè½¦è¾†
âŒ æ­£å¸¸å˜é“ã€è½¬å¼¯ã€è¶…è½¦
âŒ æ­£å¸¸é€šè¿‡è·¯å£çš„è½¦æµ
âŒ ä»…ä»…æ˜¯"ä½ç½®é è¿‘"ä½†æ— æ¥è§¦è¯æ®

ã€ç›®æ ‡IDä½¿ç”¨è§„åˆ™ã€‘
- å¿…é¡»ä½¿ç”¨å›¾ç‰‡ä¸­æ ‡æ³¨çš„çœŸå®IDï¼ˆç»¿è‰²æ¡†å·¦ä¸Šè§’"ID:X"ï¼‰
- ä¸¥ç¦ç¼–é€ ä¸å­˜åœ¨çš„ID

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{
  "verdict": "YES|POST_EVENT_ONLY|UNCERTAIN|NO",
  "confidence": 0.0-1.0,
  "evidence_frames": ["frame_03", "frame_07"],
  "impact_description": "è¯æ®æè¿°ï¼ˆYESæ—¶æè¿°ç¢°æ’è¿‡ç¨‹ï¼›POST_EVENT_ONLYæ—¶æè¿°åæœï¼›UNCERTAINæ—¶è¯´æ˜åŸå› ï¼‰",
  "participants": "car-car|car-bike|car-ped|multi|other",
  "has_accident": true/false,
  "post_event_indicators": ["å€’åœ°äººå‘˜", "è½¦è¾†å˜å½¢", "ç¢ç‰‡æ•£è½"],
  "accidents": [
    {
      "type": "äº‹æ•…ç±»å‹",
      "confidence": 0.0-1.0,
      "tracks": [æ¶‰åŠçš„ç›®æ ‡IDåˆ—è¡¨],
      "collision_time_in_clip": "clipå†…æ—¶é—´ï¼ˆç§’ï¼‰æˆ–'æœªæ•æ‰åˆ°ç¢°æ’ç¬é—´'",
      "collision_time_in_video": "åŸè§†é¢‘æ—¶é—´ï¼ˆç§’ï¼‰æˆ–'æœªæ•æ‰åˆ°ç¢°æ’ç¬é—´'",
      "collision_real_time": "ç”»é¢æ°´å°æ—¶é—´",
      "evidence": "åˆ¤æ–­ä¾æ®",
      "severity": "è½»å¾®/ä¸€èˆ¬/ä¸¥é‡",
      "behavior_before": "äº‹æ•…å‰è¡Œä¸ºï¼ˆå¦‚å¯è§ï¼‰",
      "behavior_after": "äº‹æ•…åè¡Œä¸º",
      "trajectory_description": "è½¨è¿¹æè¿°",
      "description": "äº‹æ•…è¯¦ç»†æè¿°"
    }
  ],
  "text_summary": "å®Œæ•´æè¿°"
}

æ³¨æ„ï¼š
- verdict=YESæ—¶ï¼Œhas_accident=trueï¼Œaccidentsæ•°ç»„éç©ºï¼Œæè¿°ç¢°æ’è¿‡ç¨‹
- verdict=POST_EVENT_ONLYæ—¶ï¼Œhas_accident=trueï¼Œaccidentsæ•°ç»„éç©ºï¼Œæè¿°åæœï¼Œcollision_timeå¡«"æœªæ•æ‰åˆ°ç¢°æ’ç¬é—´"
- verdict=UNCERTAINæ—¶ï¼Œhas_accident=falseï¼Œä½†éœ€åœ¨impact_descriptionè¯´æ˜ä¸ç¡®å®šåŸå› 
- verdict=NOæ—¶ï¼Œhas_accident=falseï¼Œaccidentsæ•°ç»„ä¸ºç©º
"""


def create_mock_vlm_response(mock_verdict: str = "NO") -> Dict:
    """
    åˆ›å»ºæ¨¡æ‹ŸVLMå“åº”ï¼ˆç”¨äºæµ‹è¯•ï¼‰

    Args:
        mock_verdict: "YES", "NO", "UNCERTAIN", æˆ– "POST_EVENT_ONLY"

    Returns:
        æ¨¡æ‹Ÿçš„VLMå“åº”å­—å…¸
    """
    verdict_upper = mock_verdict.upper()

    if verdict_upper == "YES":
        return {
            "verdict": "YES",
            "confidence": 0.85,
            "evidence_frames": ["frame_05"],
            "impact_description": "[MOCK] æ¨¡æ‹Ÿäº‹æ•…æ£€æµ‹ - å¯è§ç¢°æ’è¿‡ç¨‹",
            "participants": "car-bike",
            "has_accident": True,
            "post_event_indicators": [],
            "accidents": [{
                "type": "vehicle_to_bike_accident",
                "confidence": 0.85,
                "tracks": [1, 2],
                "collision_time_in_clip": "5.0",
                "evidence": "[MOCK] æ¨¡æ‹Ÿç¢°æ’è¯æ®",
                "severity": "ä¸€èˆ¬",
            }],
            "text_summary": "[MOCK] æ¨¡æ‹ŸVLMå“åº” - æ£€æµ‹åˆ°äº‹æ•…ï¼ˆå®Œæ•´è¿‡ç¨‹ï¼‰",
        }
    elif verdict_upper == "POST_EVENT_ONLY":
        return {
            "verdict": "POST_EVENT_ONLY",
            "confidence": 0.75,
            "evidence_frames": ["frame_01", "frame_02"],
            "impact_description": "[MOCK] æ¨¡æ‹Ÿäº‹æ•…åæœç‰‡æ®µ - æœªçœ‹åˆ°ç¢°æ’ä½†æœ‰å€’åœ°äººå‘˜",
            "participants": "car-bike",
            "has_accident": True,
            "post_event_indicators": ["å€’åœ°äººå‘˜", "è½¦è¾†å¼‚å¸¸åœç•™"],
            "accidents": [{
                "type": "vehicle_to_bike_accident",
                "confidence": 0.75,
                "tracks": [1, 2],
                "collision_time_in_clip": "æœªæ•æ‰åˆ°ç¢°æ’ç¬é—´",
                "evidence": "[MOCK] å¯è§äº‹æ•…åæœä½†ç¢°æ’ç¬é—´ä¸åœ¨ç”»é¢ä¸­",
                "severity": "ä¸€èˆ¬",
            }],
            "text_summary": "[MOCK] æ¨¡æ‹ŸVLMå“åº” - äº‹æ•…åæœç‰‡æ®µï¼ˆéœ€è¡¥å……æ›´æ—©ç”»é¢ï¼‰",
        }
    elif verdict_upper == "UNCERTAIN":
        return {
            "verdict": "UNCERTAIN",
            "confidence": 0.5,
            "evidence_frames": [],
            "impact_description": "[MOCK] æ¨¡æ‹Ÿä¸ç¡®å®šçŠ¶æ€ - ç”»é¢ä¸æ¸…æ™°",
            "participants": "other",
            "has_accident": False,
            "post_event_indicators": [],
            "accidents": [],
            "text_summary": "[MOCK] æ¨¡æ‹ŸVLMå“åº” - ä¸ç¡®å®š",
        }
    else:  # NO
        return {
            "verdict": "NO",
            "confidence": 0.9,
            "evidence_frames": [],
            "impact_description": "[MOCK] æ¨¡æ‹Ÿæ— äº‹æ•… - æ­£å¸¸äº¤é€šåœºæ™¯",
            "participants": "other",
            "has_accident": False,
            "post_event_indicators": [],
            "accidents": [],
            "text_summary": "[MOCK] æ¨¡æ‹ŸVLMå“åº” - æ— äº‹æ•…",
        }


def parse_vlm_response_with_verdict(parsed: Dict, strict_evidence_required: bool = True) -> Dict:
    """
    è§£æVLMå“åº”ï¼Œç¡®ä¿åŒ…å«verdictå­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰

    æ”¯æŒå››æ€ï¼šYES / POST_EVENT_ONLY / UNCERTAIN / NO

    Args:
        parsed: åŸå§‹è§£æçš„VLMå“åº”
        strict_evidence_required: å¦‚æœä¸ºTrueï¼ŒNOé«˜ç½®ä¿¡åº¦ä½†æ— evidence_framesæ—¶é™çº§ä¸ºUNCERTAIN

    Returns:
        åŒ…å«verdictå­—æ®µçš„å“åº”
    """
    VALID_VERDICTS = ("YES", "POST_EVENT_ONLY", "UNCERTAIN", "NO")

    # å·²æœ‰verdictå­—æ®µ
    if "verdict" in parsed:
        verdict = parsed["verdict"].upper()
        if verdict not in VALID_VERDICTS:
            verdict = "UNCERTAIN"
        parsed["verdict"] = verdict

        # ç¡®ä¿has_accidentä¸verdictä¸€è‡´
        if verdict in ("YES", "POST_EVENT_ONLY"):
            parsed["has_accident"] = True
        else:
            parsed["has_accident"] = False

        # ä¸¥æ ¼è¯æ®è¦æ±‚ï¼šNOé«˜ç½®ä¿¡ä½†æ— è¯æ®æ—¶é™çº§
        if strict_evidence_required and verdict == "NO":
            conf = parsed.get("confidence", 0.0)
            evidence = parsed.get("evidence_frames", [])
            impact = parsed.get("impact_description", "")

            if conf >= 0.9 and not evidence and not impact:
                parsed["verdict"] = "UNCERTAIN"
                parsed["has_accident"] = False
                parsed["downgrade_reason"] = "NO_high_conf_but_no_evidence"

        return parsed

    # æ—§æ ¼å¼ï¼šä»has_accidentè½¬æ¢
    has_accident = parsed.get("has_accident", False)
    accidents = parsed.get("accidents", [])
    summary = parsed.get("text_summary", "").lower()
    impact = parsed.get("impact_description", "").lower()

    if has_accident and accidents:
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¢°æ’æ—¶é—´è®°å½•ï¼Œåˆ¤æ–­æ˜¯YESè¿˜æ˜¯POST_EVENT_ONLY
        collision_time = None
        for acc in accidents:
            ct = acc.get("collision_time_in_clip", "")
            if ct and "æœªæ•æ‰" not in str(ct) and ct != "":
                collision_time = ct
                break

        if collision_time:
            parsed["verdict"] = "YES"
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰åæœæŒ‡æ ‡
            post_indicators = parsed.get("post_event_indicators", [])
            post_keywords = ["å€’åœ°", "å˜å½¢", "ç¢ç‰‡", "æ•£è½", "ç»•è¡Œ", "æ•‘æ´", "äº¤è­¦", "è­¦ç¤º"]
            has_post_evidence = (
                len(post_indicators) > 0 or
                any(kw in summary for kw in post_keywords) or
                any(kw in impact for kw in post_keywords)
            )
            if has_post_evidence:
                parsed["verdict"] = "POST_EVENT_ONLY"
            else:
                parsed["verdict"] = "YES"  # é»˜è®¤ä¸ºYES

        parsed["confidence"] = max(
            (a.get("confidence", 0.5) for a in accidents),
            default=0.5
        )
    else:
        # æ— äº‹æ•… - åˆ¤æ–­æ˜¯NOè¿˜æ˜¯UNCERTAINè¿˜æ˜¯POST_EVENT_ONLY
        uncertain_keywords = ["ä¸ç¡®å®š", "æ— æ³•ç¡®è®¤", "å¯èƒ½", "ç–‘ä¼¼", "unclear", "uncertain", "æ¨¡ç³Š", "é®æŒ¡"]
        post_keywords = ["å€’åœ°", "å˜å½¢", "ç¢ç‰‡", "æ•£è½", "ç»•è¡Œ", "æ•‘æ´", "äº¤è­¦", "è­¦ç¤º", "åæœ"]

        if any(kw in summary for kw in post_keywords) or any(kw in impact for kw in post_keywords):
            # æœ‰åæœä½†has_accidentä¸ºFalseï¼Œæ ‡è®°ä¸ºPOST_EVENT_ONLY
            parsed["verdict"] = "POST_EVENT_ONLY"
            parsed["has_accident"] = True
            parsed["confidence"] = 0.6
        elif any(kw in summary for kw in uncertain_keywords) or any(kw in impact for kw in uncertain_keywords):
            parsed["verdict"] = "UNCERTAIN"
            parsed["confidence"] = 0.4
        else:
            parsed["verdict"] = "NO"
            parsed["confidence"] = 0.8

    # è¡¥å……ç¼ºå¤±å­—æ®µ
    if "evidence_frames" not in parsed:
        parsed["evidence_frames"] = []
    if "impact_description" not in parsed:
        parsed["impact_description"] = parsed.get("text_summary", "")
    if "post_event_indicators" not in parsed:
        parsed["post_event_indicators"] = []
    if "participants" not in parsed:
        if accidents:
            # ä»äº‹æ•…ç±»å‹æ¨æ–­å‚ä¸è€…
            acc_type = accidents[0].get("type", "")
            if "bike" in acc_type:
                parsed["participants"] = "car-bike"
            elif "pedestrian" in acc_type:
                parsed["participants"] = "car-ped"
            elif "multi" in acc_type:
                parsed["participants"] = "multi"
            else:
                parsed["participants"] = "car-car"
        else:
            parsed["participants"] = "other"

    return parsed


def image_to_base64_url(path: str, max_width: int = None, quality: int = None) -> str:
    """
    å°†å›¾åƒè½¬æ¢ä¸ºbase64 URLï¼Œæ”¯æŒå¯é€‰çš„å‹ç¼©

    Args:
        path: å›¾åƒæ–‡ä»¶è·¯å¾„
        max_width: æœ€å¤§å®½åº¦ï¼ˆåƒç´ ï¼‰ï¼Œè¶…è¿‡åˆ™ç¼©æ”¾
        quality: JPEGå‹ç¼©è´¨é‡ï¼ˆ1-100ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸å‹ç¼©
    """
    if max_width is None and quality is None:
        # æ— å‹ç¼©ï¼Œç›´æ¥è¯»å–
        with open(path, "rb") as f:
            data = base64.b64encode(f.read()).decode("utf-8")
        return f"data:image/jpeg;base64,{data}"

    # éœ€è¦å‹ç¼©å¤„ç†
    from PIL import Image
    import io

    img = Image.open(path)
    original_size = os.path.getsize(path)

    # ç¼©æ”¾å¤„ç†
    if max_width and img.width > max_width:
        ratio = max_width / img.width
        new_height = int(img.height * ratio)
        img = img.resize((max_width, new_height), Image.LANCZOS)

    # è½¬æ¢ä¸ºRGBï¼ˆå»é™¤alphaé€šé“ï¼‰
    if img.mode in ('RGBA', 'P'):
        img = img.convert('RGB')

    # å‹ç¼©å¹¶ç¼–ç 
    buffer = io.BytesIO()
    save_quality = quality or 85
    img.save(buffer, format='JPEG', quality=save_quality, optimize=True)
    compressed_data = buffer.getvalue()
    compressed_size = len(compressed_data)

    # æ‰“å°å‹ç¼©æ•ˆæœï¼ˆä»…é¦–æ¬¡ï¼‰
    compression_ratio = (1 - compressed_size / original_size) * 100
    _safe_print(f"[å›¾åƒå‹ç¼©] {os.path.basename(path)}: {original_size//1024}KB â†’ {compressed_size//1024}KB (èŠ‚çœ{compression_ratio:.1f}%)")

    data = base64.b64encode(compressed_data).decode("utf-8")
    return f"data:image/jpeg;base64,{data}"


class VLMClient:
    def __init__(self, config: VLMConfig, api_key: Optional[str] = None):
        key = api_key or os.getenv("DASHSCOPE_API_KEY")
        if not key:
            raise ValueError("ç¼ºå°‘ DASHSCOPE_API_KEYï¼Œæ— æ³•è°ƒç”¨äº‘ç«¯ VLM")
        base_url = os.getenv("DASHSCOPE_BASE_URL")
        if not base_url:
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.client = OpenAI(api_key=key, base_url=base_url)
        # å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰
        self.async_client = AsyncOpenAI(api_key=key, base_url=base_url)
        self.config = config

    def build_user_prompt(
        self,
        intersection_info: Dict,
        tracks_text: str,
        traffic_light_text: str,
        user_query: str,
    ) -> str:
        return f"""
## è·¯å£é…ç½®
- è·¯å£ç±»å‹: {intersection_info.get('intersection_type', 'æœªçŸ¥')}
- è½¦é“æ–¹å‘è¯´æ˜: {intersection_info.get('direction_description', 'æœªæä¾›')}
- éæœºåŠ¨è½¦é“ä½ç½®: {intersection_info.get('bike_lane_description', 'æœªæä¾›')}

## ç»“æ„åŒ–è½¨è¿¹æ¦‚è¦ï¼ˆä»…ä¾›å‚è€ƒï¼‰
æ³¨æ„ï¼šä»¥ä¸‹è½¨è¿¹å¯èƒ½åŒ…å«åŒä¸€ä¸ªç›®æ ‡çš„ä¸åŒIDï¼Œæˆ–åŒ…å«è·¯è¿‡ä½†æœªè¿æ³•çš„ç›®æ ‡ï¼Œè¯·ä»”ç»†ç”„åˆ«ã€‚
{tracks_text or 'æš‚æ— è½¨è¿¹æ•°æ®'}

## ä¿¡å·ç¯çŠ¶æ€ï¼ˆè‹¥æ— å¯ä¸å†™ï¼‰
{traffic_light_text or 'æœªæ£€æµ‹åˆ°ä¿¡å·ç¯çŠ¶æ€'}

## ç”¨æˆ·æ£€ç´¢æ„å›¾
{user_query}

è¯·ç»“åˆé™„å¸¦çš„æ ‡æ³¨å›¾ç‰‡è¿›è¡Œåˆ†æï¼Œä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡ºã€‚

é‡è¦æç¤ºï¼š
1. è¯·ä»”ç»†è§‚å¯Ÿæ ‡æ³¨å¸§ä¸­æ¯ä¸ªIDçš„å®é™…è¡Œä¸º
2. åªæœ‰ç¡®å®è¿åäº¤é€šè§„åˆ™çš„ç›®æ ‡æ‰åº”è®¡å…¥violationsæ•°ç»„
3. å¯¹äºåœ¨æœºåŠ¨è½¦é“ä½†æœªè¿è§„çš„ç›®æ ‡ï¼ˆå¦‚æ­£å¸¸è¡Œé©¶ã€æœªå½±å“äº¤é€šï¼‰ï¼Œä¸åº”è®¡å…¥
4. å¦‚æœè½¨è¿¹æ–‡æœ¬æ˜¾ç¤º"å«Xä¸ªè½¨è¿¹"ï¼Œè¯·ä»”ç»†ç”„åˆ«è¿™æ˜¯åŒä¸€ä¸ªç›®æ ‡è¿˜æ˜¯å¤šä¸ªä¸åŒç›®æ ‡
5. å½“ä¸ç¡®å®šæ—¶ï¼Œå®å¯å°‘æŠ¥ä¹Ÿä¸è¦è¯¯æŠ¥
"""

    def analyze(
        self,
        annotated_images: List[str],
        intersection_info: Dict,
        tracks_text: str,
        traffic_light_text: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
    ) -> Dict:
        user_prompt = self.build_user_prompt(intersection_info, tracks_text, traffic_light_text, user_query)
        contents: List[Dict] = [{"type": "text", "text": user_prompt}]

        # P0ä¼˜åŒ–ï¼šå›¾åƒå‹ç¼©
        max_width = self.config.image_max_width if self.config.compress_images else None
        quality = self.config.image_quality if self.config.compress_images else None

        for img_path in annotated_images[: self.config.annotated_frames_per_clip]:
            contents.append(
                {
                    "type": "image_url",
                    "image_url": {"url": image_to_base64_url(img_path, max_width=max_width, quality=quality)},
                }
            )

        messages = [
            {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
            {"role": "user", "content": contents},
        ]

        # æ‰“å°å‘é€ç»™VLMçš„æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰
        _safe_print("\n" + "="*60)
        _safe_print("[VLMè¯·æ±‚] è¿æ³•æ£€æµ‹æ¨¡å¼")
        _safe_print("="*60)
        _safe_print(f"[æ¨¡å‹]: {self.config.model}")
        _safe_print(f"[æ¸©åº¦]: {self.config.temperature}")
        _safe_print(f"[å›¾ç‰‡æ•°é‡]: {len(annotated_images[:self.config.annotated_frames_per_clip])}")
        _safe_print(f"[å›¾ç‰‡è·¯å¾„]: {annotated_images[:self.config.annotated_frames_per_clip]}")
        _safe_print("-"*60)
        _safe_print("[User Prompt]:")
        _safe_print(user_prompt)
        _safe_print("="*60 + "\n")

        completion = self.client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
        )
        text = completion.choices[0].message.content

        # æ‰“å°VLMè¿”å›ç»“æœ
        _safe_print("\n" + "="*60)
        _safe_print("[VLMå“åº”]")
        _safe_print("="*60)
        _safe_print(text)
        _safe_print("="*60 + "\n")

        try:
            # å°è¯•æå– JSONï¼ˆå»é™¤å¯èƒ½çš„markdownä»£ç å—åŒ…è£¹ï¼‰
            clean_text = _extract_json_from_markdown(text)
            parsed = json.loads(clean_text)
        except Exception:
            parsed = {"has_violation": False, "violations": [], "text_summary": text}

        # ä¿å­˜VLMè¯·æ±‚å’Œå“åº”æ—¥å¿—
        _save_vlm_request_log(
            mode="violation",
            model=self.config.model,
            temperature=self.config.temperature,
            system_prompt=SYSTEM_PROMPT,
            user_prompt=user_prompt,
            image_paths=annotated_images[:self.config.annotated_frames_per_clip],
            response_text=text,
            parsed_response=parsed,
            clip_info=clip_info,
        )

        return parsed

    def _parse_frame_time_from_path(self, path: str) -> Optional[float]:
        """ä»æ ‡æ³¨å¸§æ–‡ä»¶åä¸­è§£ææ—¶é—´ï¼ˆç§’ï¼‰

        æ–‡ä»¶åæ ¼å¼: camera-1_20251210_clip-e8475403_003.600_annotated.jpg
        æå–: 003.600 -> 3.6ç§’
        """
        import re
        basename = os.path.basename(path)
        # åŒ¹é… _NNN.NNN_annotated æ ¼å¼
        match = re.search(r'_(\d+\.\d+)_annotated', basename)
        if match:
            return float(match.group(1))
        return None

    def build_accident_user_prompt(
        self,
        intersection_info: Dict,
        tracks_text: str,
        traffic_light_text: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
        annotated_images: Optional[List[str]] = None,
    ) -> str:
        """äº‹æ•…æ£€ç´¢æ¨¡å¼ä¸“ç”¨çš„ User Prompt - æ›´ç§¯æä¸»åŠ¨åœ°å¯»æ‰¾äº‹æ•…è¿¹è±¡"""

        # æ„å»ºclipæ—¶é—´ä¿¡æ¯
        clip_time_info = ""
        clip_start = 0.0
        if clip_info:
            clip_duration = clip_info.get('duration', 0)
            clip_start = clip_info.get('start_time', 0)
            clip_end = clip_info.get('end_time', 0)
            clip_time_info = f"""
## è§†é¢‘ç‰‡æ®µæ—¶é—´ä¿¡æ¯ï¼ˆâš ï¸é‡è¦ï¼šç”¨äºè®¡ç®—åŸè§†é¢‘ç»å¯¹æ—¶é—´ï¼‰
- ç‰‡æ®µæ—¶é•¿: {clip_duration:.1f}ç§’
- **ç‰‡æ®µèµ·å§‹æ—¶é—´ï¼ˆåŸè§†é¢‘ï¼‰: {clip_start:.1f}ç§’** â† ç”¨äºè®¡ç®—collision_time_in_video
- ç‰‡æ®µç»“æŸæ—¶é—´ï¼ˆåŸè§†é¢‘ï¼‰: {clip_end:.1f}ç§’
"""

        # æ„å»ºå¸§æ—¶é—´æ˜ å°„ï¼ŒåŒæ—¶è®¡ç®—åŸè§†é¢‘ç»å¯¹æ—¶é—´
        frame_time_info = ""
        if annotated_images:
            frame_times = []
            for i, img_path in enumerate(annotated_images):
                frame_time = self._parse_frame_time_from_path(img_path)
                if frame_time is not None:
                    video_time = clip_start + frame_time
                    frame_times.append(f"å›¾ç‰‡{i+1}: clipå†…{frame_time:.1f}ç§’ â†’ åŸè§†é¢‘{video_time:.1f}ç§’")
            if frame_times:
                frame_time_info = f"""
## å›¾ç‰‡æ—¶é—´å¯¹åº”å…³ç³»ï¼ˆâš ï¸é‡è¦ï¼šç”¨äºå¡«å†™æ—¶é—´å­—æ®µï¼‰
{chr(10).join(frame_times)}
"""

        return f"""
## è·¯å£é…ç½®
- è·¯å£ç±»å‹: {intersection_info.get('intersection_type', 'æœªçŸ¥')}
- è½¦é“æ–¹å‘è¯´æ˜: {intersection_info.get('direction_description', 'æœªæä¾›')}
- éæœºåŠ¨è½¦é“ä½ç½®: {intersection_info.get('bike_lane_description', 'æœªæä¾›')}
{clip_time_info}{frame_time_info}
## ç»“æ„åŒ–è½¨è¿¹æ¦‚è¦
{tracks_text or 'æš‚æ— è½¨è¿¹æ•°æ®'}

## ä¿¡å·ç¯çŠ¶æ€
{traffic_light_text or 'æœªæ£€æµ‹åˆ°ä¿¡å·ç¯çŠ¶æ€'}

## ç”¨æˆ·æ£€ç´¢æ„å›¾
{user_query}

è¯·ä»”ç»†è§‚å¯Ÿé™„å¸¦çš„æ ‡æ³¨å›¾ç‰‡ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨**ç¡®å®šçš„äº¤é€šäº‹æ•…è¯æ®**ã€‚

ã€éœ€è¦ç¡®è®¤çš„ç¡¬æ€§è¯æ®ã€‘ï¼ˆå¿…é¡»è‡³å°‘æ»¡è¶³ä¸€é¡¹ï¼‰
1. æ˜¯å¦å¯è§ä¸¤ä¸ªç›®æ ‡å‘ç”Ÿæ˜æ˜¾ç‰©ç†ç¢°æ’/æ¥è§¦ï¼Ÿ
2. æ˜¯å¦æœ‰äººå‘˜æ˜ç¡®å€’åœ°ä¸èµ·ï¼ˆèººåœ¨åœ°é¢ï¼‰ï¼Ÿ
3. æ˜¯å¦æœ‰è½¦è¾†æ˜æ˜¾æŸåå˜å½¢ï¼Ÿ
4. æ˜¯å¦æœ‰è·¯é¢æ•£è½ç¢ç‰‡/æ¶²ä½“æ³„æ¼ï¼Ÿ
5. æ˜¯å¦æœ‰äº¤è­¦/æ•‘æ´äººå‘˜æ­£åœ¨ç°åœºå¤„ç†ï¼Ÿ
6. æ˜¯å¦æœ‰ä¸‰è§’è­¦ç¤ºç‰Œã€è­¦ç¤ºé”¥æ¡¶ç­‰è­¦ç¤ºè®¾ç½®ï¼Ÿ

ã€æ­£å¸¸äº¤é€šåœºæ™¯æ’é™¤ã€‘âš ï¸
- æ­£å¸¸ç­‰çº¢ç¯æ’é˜Ÿçš„è½¦è¾† â‰  äº‹æ•…
- æ­£å¸¸å˜é“ã€è½¬å¼¯çš„è½¦è¾† â‰  äº‹æ•…
- è½¦è¾†ä½ç½®é è¿‘ä½†æ— ç¢°æ’è¯æ® â‰  äº‹æ•…
- æ­£å¸¸é€šè¿‡è·¯å£çš„è½¦æµ â‰  äº‹æ•…

âš ï¸ æ²¡æœ‰ç¡¬æ€§è¯æ®æ—¶ï¼Œè¯·ä¸è¦æŠ¥å‘Šä¸ºäº‹æ•…ã€‚å‡†ç¡®æ€§ä¼˜å…ˆï¼

ã€ç›®æ ‡IDä½¿ç”¨è¯´æ˜ã€‘â­â­â­ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
- æ¯å¼ å›¾ç‰‡ä¸Šçš„ç»¿è‰²æ£€æµ‹æ¡†å·¦ä¸Šè§’éƒ½æœ‰"ID:X"æ ‡ç­¾ï¼ˆç™½è‰²æ–‡å­—é»‘è‰²èƒŒæ™¯ï¼‰
- å¡«å†™"tracks"å­—æ®µæ—¶ï¼Œå¿…é¡»ä½¿ç”¨å›¾ç‰‡ä¸­æ˜¾ç¤ºçš„çœŸå®IDå·
- ä¸Šæ–¹"ç»“æ„åŒ–è½¨è¿¹æ¦‚è¦"ä¹Ÿåˆ—å‡ºäº†å„ã€ID:Xã€‘çš„è½¨è¿¹ä¿¡æ¯ï¼Œå¯å‚è€ƒç¡®è®¤
- **ä¸¥ç¦ç¼–é€ ä¸å­˜åœ¨çš„ID**ï¼ˆå¦‚127ã€148ã€3104.1ç­‰ï¼‰

ã€æ—¶é—´å­—æ®µå¡«å†™è¯´æ˜ã€‘â­â­â­
- collision_time_in_clip: ä½¿ç”¨ä¸Šæ–¹"å›¾ç‰‡æ—¶é—´å¯¹åº”å…³ç³»"ä¸­çš„"clipå†…Xç§’"
- collision_time_in_video: ä½¿ç”¨ä¸Šæ–¹"å›¾ç‰‡æ—¶é—´å¯¹åº”å…³ç³»"ä¸­çš„"åŸè§†é¢‘Xç§’"
- collision_real_time: ä»”ç»†è§‚å¯Ÿå›¾ç‰‡ä¸­çš„**è§†é¢‘æ°´å°æ—¶é—´**ï¼ˆé€šå¸¸åœ¨ç”»é¢è§’è½æ˜¾ç¤ºå¹´æœˆæ—¥æ—¶åˆ†ç§’ï¼‰ï¼Œå¦‚"2025-10-17 07:47:02"

è¯·ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚
"""

    def analyze_accident(
        self,
        annotated_images: List[str],
        intersection_info: Dict,
        tracks_text: str,
        traffic_light_text: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
    ) -> Dict:
        """äº‹æ•…æ£€ç´¢æ¨¡å¼ä¸“ç”¨åˆ†ææ–¹æ³•"""
        # ä½¿ç”¨äº‹æ•…ä¸“ç”¨å¸§æ•°é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        frames_limit = getattr(self.config, 'accident_frames_per_clip', self.config.annotated_frames_per_clip)
        # ä¼ é€’clip_infoå’Œannotated_imagesä»¥ä¾¿ç”Ÿæˆå¸§æ—¶é—´æ˜ å°„
        images_to_send = annotated_images[:frames_limit]
        user_prompt = self.build_accident_user_prompt(
            intersection_info, tracks_text, traffic_light_text, user_query,
            clip_info=clip_info,
            annotated_images=images_to_send,
        )

        contents: List[Dict] = [{"type": "text", "text": user_prompt}]

        # P0ä¼˜åŒ–ï¼šå›¾åƒå‹ç¼©
        max_width = self.config.image_max_width if self.config.compress_images else None
        quality = self.config.image_quality if self.config.compress_images else None

        for img_path in annotated_images[:frames_limit]:
            contents.append(
                {
                    "type": "image_url",
                    "image_url": {"url": image_to_base64_url(img_path, max_width=max_width, quality=quality)},
                }
            )

        messages = [
            {"role": "system", "content": [{"type": "text", "text": ACCIDENT_SYSTEM_PROMPT}]},
            {"role": "user", "content": contents},
        ]

        # æ‰“å°å‘é€ç»™VLMçš„æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰
        _safe_print("\n" + "="*60)
        _safe_print("[VLMè¯·æ±‚] äº‹æ•…æ£€ç´¢æ¨¡å¼")
        _safe_print("="*60)
        _safe_print(f"[æ¨¡å‹]: {self.config.model}")
        _safe_print(f"[æ¸©åº¦]: {self.config.temperature}")
        _safe_print(f"[å›¾ç‰‡æ•°é‡]: {len(annotated_images[:frames_limit])}")
        _safe_print(f"[å›¾ç‰‡è·¯å¾„]: {annotated_images[:frames_limit]}")
        _safe_print("-"*60)
        _safe_print("[User Prompt]:")
        _safe_print(user_prompt)
        _safe_print("="*60 + "\n")

        completion = self.client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
        )
        text = completion.choices[0].message.content

        # æ‰“å°VLMè¿”å›ç»“æœ
        _safe_print("\n" + "="*60)
        _safe_print("[VLMå“åº”] äº‹æ•…æ£€ç´¢æ¨¡å¼")
        _safe_print("="*60)
        _safe_print(text)
        _safe_print("="*60 + "\n")

        try:
            # å»é™¤å¯èƒ½çš„markdownä»£ç å—åŒ…è£¹
            clean_text = _extract_json_from_markdown(text)
            parsed = json.loads(clean_text)
        except Exception:
            parsed = {"has_accident": False, "accidents": [], "text_summary": text}

        # ä¿å­˜VLMè¯·æ±‚å’Œå“åº”æ—¥å¿—
        _save_vlm_request_log(
            mode="accident",
            model=self.config.model,
            temperature=self.config.temperature,
            system_prompt=ACCIDENT_SYSTEM_PROMPT,
            user_prompt=user_prompt,
            image_paths=annotated_images[:frames_limit],
            response_text=text,
            parsed_response=parsed,
            clip_info=clip_info,
        )

        return parsed

    # ==================== å¼‚æ­¥å¹¶å‘VLMè°ƒç”¨ ====================

    async def analyze_accident_async(
        self,
        annotated_images: List[str],
        intersection_info: Dict,
        tracks_text: str,
        traffic_light_text: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
    ) -> Dict:
        """äº‹æ•…æ£€ç´¢æ¨¡å¼å¼‚æ­¥åˆ†ææ–¹æ³•ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰"""
        frames_limit = getattr(self.config, 'accident_frames_per_clip', self.config.annotated_frames_per_clip)
        images_to_send = annotated_images[:frames_limit]
        user_prompt = self.build_accident_user_prompt(
            intersection_info, tracks_text, traffic_light_text, user_query,
            clip_info=clip_info,
            annotated_images=images_to_send,
        )

        contents: List[Dict] = [{"type": "text", "text": user_prompt}]

        # P0ä¼˜åŒ–ï¼šå›¾åƒå‹ç¼©
        max_width = self.config.image_max_width if self.config.compress_images else None
        quality = self.config.image_quality if self.config.compress_images else None

        for img_path in annotated_images[:frames_limit]:
            contents.append(
                {
                    "type": "image_url",
                    "image_url": {"url": image_to_base64_url(img_path, max_width=max_width, quality=quality)},
                }
            )

        messages = [
            {"role": "system", "content": [{"type": "text", "text": ACCIDENT_SYSTEM_PROMPT}]},
            {"role": "user", "content": contents},
        ]

        _safe_print(f"[VLMå¼‚æ­¥] äº‹æ•…æ£€ç´¢æ¨¡å¼ - å›¾ç‰‡æ•°:{len(images_to_send)}")

        # å¼‚æ­¥è°ƒç”¨VLM
        completion = await self.async_client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
        )
        text = completion.choices[0].message.content

        _safe_print(f"[VLMå¼‚æ­¥] å“åº”å®Œæˆ - {len(text)} å­—ç¬¦")

        try:
            # å»é™¤å¯èƒ½çš„markdownä»£ç å—åŒ…è£¹
            clean_text = _extract_json_from_markdown(text)
            parsed = json.loads(clean_text)
        except Exception:
            parsed = {"has_accident": False, "accidents": [], "text_summary": text}

        # ä¿å­˜VLMè¯·æ±‚å’Œå“åº”æ—¥å¿—
        _save_vlm_request_log(
            mode="accident_async",
            model=self.config.model,
            temperature=self.config.temperature,
            system_prompt=ACCIDENT_SYSTEM_PROMPT,
            user_prompt=user_prompt,
            image_paths=annotated_images[:frames_limit],
            response_text=text,
            parsed_response=parsed,
            clip_info=clip_info,
        )

        return parsed

    async def batch_analyze_accidents_async(
        self,
        clips_data: List[Dict],
        max_concurrent: int = 2
    ) -> List[Dict]:
        """
        æ‰¹é‡å¼‚æ­¥åˆ†æå¤šä¸ªclips

        Args:
            clips_data: æ¯ä¸ªå…ƒç´ åŒ…å«åˆ†ææ‰€éœ€çš„å…¨éƒ¨æ•°æ®
                {
                    "annotated_images": List[str],
                    "intersection_info": Dict,
                    "tracks_text": str,
                    "traffic_light_text": str,
                    "user_query": str,
                    "clip_info": Optional[Dict]
                }
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            åˆ†æç»“æœåˆ—è¡¨ï¼ˆä¸è¾“å…¥é¡ºåºå¯¹åº”ï¼‰
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def analyze_one(data: Dict, index: int) -> Dict:
            async with semaphore:
                _safe_print(f"[VLMæ‰¹é‡] å¼€å§‹åˆ†æ clip #{index}")
                try:
                    result = await self.analyze_accident_async(
                        annotated_images=data["annotated_images"],
                        intersection_info=data["intersection_info"],
                        tracks_text=data["tracks_text"],
                        traffic_light_text=data["traffic_light_text"],
                        user_query=data["user_query"],
                        clip_info=data.get("clip_info"),
                    )
                    result["_clip_index"] = index
                    _safe_print(f"[VLMæ‰¹é‡] clip #{index} å®Œæˆ")
                    return result
                except Exception as e:
                    _safe_print(f"[VLMæ‰¹é‡] clip #{index} å¤±è´¥: {e}")
                    return {"has_accident": False, "accidents": [], "error": str(e), "_clip_index": index}

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
        tasks = [analyze_one(data, i) for i, data in enumerate(clips_data)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                final_results.append({"has_accident": False, "accidents": [], "error": str(result), "_clip_index": i})
            else:
                final_results.append(result)

        return final_results

    def batch_analyze_accidents_sync(
        self,
        clips_data: List[Dict],
        max_concurrent: int = 2
    ) -> List[Dict]:
        """
        åŒæ­¥å°è£…çš„æ‰¹é‡åˆ†ææ–¹æ³•ï¼ˆæ–¹ä¾¿ä»åŒæ­¥ä»£ç ä¸­è°ƒç”¨ï¼‰

        å†…éƒ¨ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥æ‰¹é‡åˆ†æ
        """
        _safe_print(f"[VLMæ‰¹é‡åŒæ­¥] å¯åŠ¨ {len(clips_data)} ä¸ªclipsçš„å¹¶å‘åˆ†æï¼Œæœ€å¤§å¹¶å‘: {max_concurrent}")

        try:
            # å°è¯•è·å–ç°æœ‰äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœå·²åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯ä¸­ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        asyncio.run,
                        self.batch_analyze_accidents_async(clips_data, max_concurrent)
                    )
                    return future.result()
            else:
                return loop.run_until_complete(
                    self.batch_analyze_accidents_async(clips_data, max_concurrent)
                )
        except RuntimeError:
            # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
            return asyncio.run(
                self.batch_analyze_accidents_async(clips_data, max_concurrent)
            )

    # ==================== æ¸è¿›å¼VLMåˆ†æï¼ˆæ— æ¡†åŸå›¾+å…ƒæ•°æ®ï¼‰ ====================

    # æ¸è¿›å¼VLMä¸“ç”¨ System Promptï¼ˆä½¿ç”¨æ— æ¡†åŸå›¾+æ–‡æœ¬å…ƒæ•°æ®ï¼‰
    PROGRESSIVE_VLM_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸“ä¸šçš„äº¤é€šäº‹æ•…é‰´å®šä¸“å®¶ï¼Œè´Ÿè´£ç²¾å‡†è¯†åˆ«é“è·¯äº¤é€šäº‹æ•…ã€‚

**æ ¸å¿ƒåŸåˆ™ï¼šè¯æ®é©±åŠ¨ï¼ŒåŸºäºåŸå§‹ç”»é¢åˆ¤æ–­ã€‚**

ä½ ä¼šæ”¶åˆ°ï¼š
1. **åŸå§‹ç›‘æ§å¸§å›¾ç‰‡**ï¼ˆæ— æ£€æµ‹æ¡†å åŠ ï¼Œä¿æŒç”»é¢åŸè²Œï¼‰
2. **ç»“æ„åŒ–æ£€æµ‹å…ƒæ•°æ®**ï¼ˆæ–‡æœ¬å½¢å¼çš„ç›®æ ‡æ£€æµ‹å’Œè½¨è¿¹ä¿¡æ¯ï¼‰

ã€å››æ€åˆ¤å®šè§„åˆ™ã€‘â­â­â­

**verdict = "YES"**ï¼ˆç¡®è®¤äº‹æ•… - çœ‹åˆ°ç¢°æ’è¿‡ç¨‹ï¼‰
  å¿…é¡»æ»¡è¶³ä»¥ä¸‹è‡³å°‘ä¸€é¡¹ç¡¬æ€§è¯æ®ï¼š
  - å¯è§ä¸¤ä¸ªç›®æ ‡å‘ç”Ÿæ˜æ˜¾ç‰©ç†æ¥è§¦/ç¢°æ’çš„ç¬é—´
  - ç¢°æ’ç¬é—´æœ‰å½¢å˜ã€æ•£è½ç¢ç‰‡
  - äººå‘˜è¢«æ’å€’åœ°çš„è¿‡ç¨‹
  - è½¦è¾†ç¢°æ’å¯¼è‡´æŸåå˜å½¢
  - æ³¨ï¼šè½»å¾®å‰è¹­ä¹Ÿç®—äº‹æ•…ï¼Œåªè¦èƒ½çœ‹åˆ°æ¥è§¦è¿‡ç¨‹

**verdict = "POST_EVENT_ONLY"**ï¼ˆä»…åæœ - æœªçœ‹åˆ°ç¢°æ’ä½†æœ‰æ˜ç¡®åæœï¼‰
  é€‚ç”¨åœºæ™¯ï¼š
  - æœªçœ‹åˆ°ç¢°æ’/æ¥è§¦çš„ç¬é—´
  - ä½†çœ‹åˆ°æ˜ç¡®çš„äº‹æ•…åæœï¼šäººå‘˜å€’åœ°ã€è½¦è¾†å—æŸã€ç¢ç‰‡æ•£è½ã€æ•‘æ´ç°åœº

**verdict = "UNCERTAIN"**ï¼ˆä¸ç¡®å®šï¼Œéœ€äººå·¥å¤æ ¸ï¼‰
  ä»¥ä¸‹æƒ…å†µå¿…é¡»é€‰æ‹©UNCERTAINï¼š
  - ç”»é¢æ¨¡ç³Š/é®æŒ¡æ— æ³•çœ‹æ¸…å…³é”®ç¬é—´
  - çœ‹èµ·æ¥æœ‰å±é™©ä½†æ— æ³•ç¡®è®¤æ˜¯å¦å‘ç”Ÿæ¥è§¦
  - ç›®æ ‡ä½ç½®éå¸¸æ¥è¿‘ä½†ç¢°æ’ç¬é—´ä¸åœ¨ç”»é¢ä¸­
  - è¯æ®ä¸å……åˆ†ï¼Œæ— æ³•åšå‡ºåˆ¤æ–­

**verdict = "NO"**ï¼ˆç¡®è®¤æ— äº‹æ•…ï¼‰
  å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š
  - æ²¡æœ‰çœ‹åˆ°ä»»ä½•ç¢°æ’/æ¥è§¦è¯æ®
  - æ²¡æœ‰çœ‹åˆ°ä»»ä½•äº‹æ•…åæœ
  - æ‰€æœ‰ç›®æ ‡è¡Œä¸ºæ­£å¸¸

âš ï¸ **é‡è¦**ï¼š
- ç»“åˆå…ƒæ•°æ®ä¸­çš„è·ç¦»ä¿¡æ¯å’Œè½¨è¿¹å˜åŒ–æ¥è¾…åŠ©åˆ¤æ–­
- å…ƒæ•°æ®ä¸­"æœ€è¿‘è·ç¦»"å°äº50åƒç´ æ—¶éœ€ç‰¹åˆ«å…³æ³¨
- è½¨è¿¹çªç„¶ä¸­æ–­å¯èƒ½æ„å‘³ç€ç¢°æ’å‘ç”Ÿ

**ğŸ”´ ç‰©ç†æ¥è§¦ä¼˜å…ˆæ£€æµ‹è§„åˆ™**ï¼š
- è¯·ä¼˜å…ˆæ£€æµ‹æ˜¯å¦å­˜åœ¨ç‰©ç†æ¥è§¦ï¼ˆç¢°æ’ã€åˆ®æ“¦ã€æŒ¤å‹ï¼‰ï¼Œå³ä½¿å½±å“çœ‹èµ·æ¥è½»å¾®
- è½¦è¾†å¼‚å¸¸æ¥è¿‘ï¼ˆè§†è§‰è½¦è·<1ç±³æˆ–bboxé‡å >30%ï¼‰åº”åˆ¤å®šä¸ºUNCERTAINè€ŒéNO
- è½»å¾®ä¾§æ“¦/åˆ®è¹­å³ä½¿æ— æ˜æ˜¾å˜å½¢ä¹Ÿç®—äº‹æ•…çº¿ç´¢
- å¦‚æœæ— æ³•ç¡®å®šæ˜¯å¦æœ‰æ¥è§¦ï¼Œåˆ¤UNCERTAINè€ŒéNO

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{
  "verdict": "YES|POST_EVENT_ONLY|UNCERTAIN|NO",
  "confidence": 0.0-1.0,
  "evidence_frames": ["frame_03", "frame_07"],
  "impact_description": "è¯æ®æè¿°",
  "participants": "car-car|car-bike|car-ped|multi|other",
  "has_accident": true/false,
  "accidents": [...],
  "text_summary": "å®Œæ•´æè¿°"
}
"""

    def build_progressive_user_prompt(
        self,
        metadata_text: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
    ) -> str:
        """æ¸è¿›å¼VLMçš„User Promptï¼ˆåŸå§‹å¸§+å…ƒæ•°æ®ï¼‰"""

        clip_time_info = ""
        short_video_hint = ""
        if clip_info:
            clip_duration = clip_info.get('duration', 0)
            clip_start = clip_info.get('start_time', 0)
            clip_end = clip_info.get('end_time', 0)
            clip_time_info = f"""
## è§†é¢‘ç‰‡æ®µæ—¶é—´ä¿¡æ¯
- ç‰‡æ®µæ—¶é•¿: {clip_duration:.1f}ç§’
- ç‰‡æ®µèµ·å§‹æ—¶é—´ï¼ˆåŸè§†é¢‘ï¼‰: {clip_start:.1f}ç§’
- ç‰‡æ®µç»“æŸæ—¶é—´ï¼ˆåŸè§†é¢‘ï¼‰: {clip_end:.1f}ç§’
"""
            # v2: çŸ­è§†é¢‘æç¤ºå¥
            if clip_duration <= 15.0:
                short_video_hint = """
âš ï¸ **çŸ­è§†é¢‘ç‰¹åˆ«æç¤º**ï¼šè¿™æ˜¯ä¸€æ®µçŸ­è§†é¢‘(<15ç§’)ï¼Œéœ€è¦æ›´ä»”ç»†æ£€æŸ¥æ¯ä¸€å¸§çš„ç»†èŠ‚å˜åŒ–ï¼Œç‰¹åˆ«å…³æ³¨ç”»é¢ä¸­äº¤é€šå‚ä¸è€…ä¹‹é—´çš„æ¥è§¦ç¬é—´ã€‚çŸ­è§†é¢‘ä¸­äº‹æ•…å‘ç”Ÿå¿«ï¼Œè¯·é€å¸§ä»”ç»†è§‚å¯Ÿã€‚
"""

        return f"""
{clip_time_info}
{short_video_hint}
{metadata_text}

## ç”¨æˆ·æ£€ç´¢æ„å›¾
{user_query}

è¯·ä»”ç»†è§‚å¯Ÿé™„å¸¦çš„**åŸå§‹ç›‘æ§å¸§å›¾ç‰‡**ï¼Œç»“åˆä¸Šæ–¹çš„æ£€æµ‹å…ƒæ•°æ®ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨äº¤é€šäº‹æ•…ã€‚

ã€åˆ†æè¦ç‚¹ã€‘
1. ä»”ç»†è§‚å¯Ÿå›¾ç‰‡ä¸­çš„è½¦è¾†å’Œè¡Œäººä½ç½®ã€å§¿æ€
2. å‚è€ƒå…ƒæ•°æ®ä¸­çš„"æœ€è¿‘è·ç¦»"å’Œ"è½¨è¿¹æ‘˜è¦"ä¿¡æ¯
3. å…³æ³¨å…ƒæ•°æ®ä¸­æ ‡è®°çš„ motion_peakã€interaction_peak å¸§
4. å¦‚æœ"æœ€è¿‘è·ç¦»"<50åƒç´ ï¼Œéœ€ç‰¹åˆ«ä»”ç»†æŸ¥çœ‹è¯¥å¸§

è¯·ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚
"""

    def analyze_progressive(
        self,
        raw_images: List[str],
        metadata_text: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
        mode: str = "FAST",
    ) -> Dict:
        """
        æ¸è¿›å¼VLMåˆ†æï¼ˆä½¿ç”¨æ— æ¡†åŸå›¾+å…ƒæ•°æ®æ–‡æœ¬ï¼‰

        Args:
            raw_images: åŸå§‹å¸§å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆæ— YOLOå åŠ ï¼‰
            metadata_text: ç»“æ„åŒ–æ£€æµ‹å…ƒæ•°æ®æ–‡æœ¬
            user_query: ç”¨æˆ·æŸ¥è¯¢
            clip_info: clipä¿¡æ¯
            mode: "FAST"(4-6å¸§) æˆ– "ESCALATED"(12-16å¸§)

        Returns:
            VLMåˆ†æç»“æœ
        """
        user_prompt = self.build_progressive_user_prompt(
            metadata_text, user_query, clip_info
        )

        contents: List[Dict] = [{"type": "text", "text": user_prompt}]

        # æ ¹æ®æ¨¡å¼ç¡®å®šå¸§æ•°é™åˆ¶
        if mode == "FAST":
            frames_limit = 6
        else:  # ESCALATED
            frames_limit = 16

        # P0ä¼˜åŒ–ï¼šå›¾åƒå‹ç¼©
        max_width = self.config.image_max_width if self.config.compress_images else None
        quality = self.config.image_quality if self.config.compress_images else None

        for img_path in raw_images[:frames_limit]:
            contents.append(
                {
                    "type": "image_url",
                    "image_url": {"url": image_to_base64_url(img_path, max_width=max_width, quality=quality)},
                }
            )

        messages = [
            {"role": "system", "content": [{"type": "text", "text": self.PROGRESSIVE_VLM_SYSTEM_PROMPT}]},
            {"role": "user", "content": contents},
        ]

        # æ‰“å°å‘é€ç»™VLMçš„æ•°æ®
        _safe_print("\n" + "="*60)
        _safe_print(f"[VLMè¯·æ±‚] æ¸è¿›å¼åˆ†æ - {mode}æ¨¡å¼")
        _safe_print("="*60)
        _safe_print(f"[æ¨¡å‹]: {self.config.model}")
        _safe_print(f"[å›¾ç‰‡æ•°é‡]: {len(raw_images[:frames_limit])} (æ— æ¡†åŸå›¾)")
        _safe_print(f"[å…ƒæ•°æ®é•¿åº¦]: {len(metadata_text)} å­—ç¬¦")
        _safe_print("="*60 + "\n")

        completion = self.client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
        )
        text = completion.choices[0].message.content

        # æ‰“å°VLMè¿”å›ç»“æœ
        _safe_print("\n" + "="*60)
        _safe_print(f"[VLMå“åº”] æ¸è¿›å¼åˆ†æ - {mode}æ¨¡å¼")
        _safe_print("="*60)
        _safe_print(text)
        _safe_print("="*60 + "\n")

        try:
            clean_text = _extract_json_from_markdown(text)
            parsed = json.loads(clean_text)
        except Exception:
            parsed = {"has_accident": False, "accidents": [], "text_summary": text}

        # ä¿å­˜VLMè¯·æ±‚å’Œå“åº”æ—¥å¿—
        _save_vlm_request_log(
            mode=f"progressive_{mode.lower()}",
            model=self.config.model,
            temperature=self.config.temperature,
            system_prompt=self.PROGRESSIVE_VLM_SYSTEM_PROMPT,
            user_prompt=user_prompt,
            image_paths=raw_images[:frames_limit],
            response_text=text,
            parsed_response=parsed,
            clip_info=clip_info,
        )

        # æ·»åŠ æ¨¡å¼æ ‡è®°
        parsed["_vlm_mode"] = mode
        parsed["_n_frames"] = len(raw_images[:frames_limit])

        return parsed

    def analyze_progressive_two_stage(
        self,
        raw_images_fast: List[str],
        raw_images_escalated: List[str],
        metadata_text_fast: str,
        metadata_text_escalated: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
        escalate_on_verdicts: List[str] = None,
        escalate_on_conflict: bool = True,
        conflict_risk_threshold: float = 0.6,
        resolution_conservative: bool = True,
        # v2æ–°å¢å‡çº§å‚æ•°
        escalate_on_low_conf_no: bool = True,
        low_conf_no_threshold: float = 0.5,
        escalate_on_high_signal: bool = True,
        high_signal_threshold: float = 0.7,
        max_signal_score: float = 0.0,
    ) -> Dict:
        """
        ä¸¤é˜¶æ®µæ¸è¿›å¼VLMåˆ†æï¼ˆv2æ”¯æŒæ›´å¤šå‡çº§è§„åˆ™ï¼‰

        Args:
            raw_images_fast: S1å¿«é€ŸéªŒè¯å¸§
            raw_images_escalated: S2å‡çº§éªŒè¯å¸§
            metadata_text_fast: S1å…ƒæ•°æ®
            metadata_text_escalated: S2å…ƒæ•°æ®
            user_query: ç”¨æˆ·æŸ¥è¯¢
            clip_info: clipä¿¡æ¯
            escalate_on_verdicts: è§¦å‘å‡çº§çš„verdictåˆ—è¡¨
            escalate_on_conflict: æ˜¯å¦åœ¨riské«˜ä½†verdict=NOæ—¶å‡çº§
            conflict_risk_threshold: conflictè§„åˆ™çš„riské˜ˆå€¼
            resolution_conservative: ä½¿ç”¨ä¿å®ˆè§£æè§„åˆ™
            escalate_on_low_conf_no: verdict=NOä½†ç½®ä¿¡åº¦ä½æ—¶å‡çº§
            low_conf_no_threshold: ä½ç½®ä¿¡åº¦NOçš„é˜ˆå€¼
            escalate_on_high_signal: é«˜ä¿¡å·åˆ†æ—¶å‡çº§
            high_signal_threshold: é«˜ä¿¡å·åˆ†é˜ˆå€¼
            max_signal_score: æœ€å¤§ä¿¡å·åˆ†ï¼ˆç”±keyframe_selectorè®¡ç®—ï¼‰

        Returns:
            åŒ…å«ä¸¤é˜¶æ®µç»“æœçš„å­—å…¸
        """
        if escalate_on_verdicts is None:
            escalate_on_verdicts = ["UNCERTAIN", "POST_EVENT_ONLY"]

        # ===== S1: å¿«é€ŸéªŒè¯ =====
        _safe_print(f"\n[æ¸è¿›å¼VLM] S1é˜¶æ®µï¼šå¿«é€ŸéªŒè¯ ({len(raw_images_fast)}å¸§)")
        result_s1 = self.analyze_progressive(
            raw_images=raw_images_fast,
            metadata_text=metadata_text_fast,
            user_query=user_query,
            clip_info=clip_info,
            mode="FAST"
        )

        # è§£æS1ç»“æœ
        result_s1 = parse_vlm_response_with_verdict(result_s1)
        verdict_s1 = result_s1.get("verdict", "NO")
        conf_s1 = result_s1.get("confidence", 0.0)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§
        need_escalation = False
        escalation_reasons = []

        # è§„åˆ™1: verdictåœ¨å‡çº§åˆ—è¡¨ä¸­
        if verdict_s1 in escalate_on_verdicts:
            need_escalation = True
            escalation_reasons.append(f"verdict={verdict_s1}")

        # è§„åˆ™2: riské«˜ä½†verdict=NOï¼ˆå†²çªï¼‰
        if escalate_on_conflict and verdict_s1 == "NO":
            clip_score = clip_info.get("clip_score", 0) if clip_info else 0
            accident_score = clip_info.get("accident_score", 0) if clip_info else 0
            risk_score = max(clip_score, accident_score)

            if risk_score >= conflict_risk_threshold:
                need_escalation = True
                escalation_reasons.append(f"conflict: risk={risk_score:.2f}>={conflict_risk_threshold}")

        # è§„åˆ™3 (v2): verdict=NOä½†ç½®ä¿¡åº¦ä½
        if escalate_on_low_conf_no and verdict_s1 == "NO" and conf_s1 < low_conf_no_threshold:
            need_escalation = True
            escalation_reasons.append(f"low_conf_no: conf={conf_s1:.2f}<{low_conf_no_threshold}")

        # è§„åˆ™4 (v2): é«˜ä¿¡å·åˆ†
        if escalate_on_high_signal and max_signal_score >= high_signal_threshold:
            need_escalation = True
            escalation_reasons.append(f"high_signal: {max_signal_score:.2f}>={high_signal_threshold}")

        escalation_reason = " | ".join(escalation_reasons) if escalation_reasons else ""

        # å¦‚æœä¸éœ€è¦å‡çº§ï¼Œç›´æ¥è¿”å›S1ç»“æœ
        if not need_escalation:
            _safe_print(f"[æ¸è¿›å¼VLM] S1å®Œæˆï¼Œæ— éœ€å‡çº§ (verdict={verdict_s1}, conf={conf_s1:.2f})")
            return {
                "verdict": verdict_s1,
                "confidence": conf_s1,
                "has_accident": result_s1.get("has_accident", False),
                "accidents": result_s1.get("accidents", []),
                "text_summary": result_s1.get("text_summary", ""),
                "escalated": False,
                "stage1_result": result_s1,
                "stage2_result": None,
                "escalation_reason": "",
                "_progressive_vlm": True,
            }

        # ===== S2: å‡çº§éªŒè¯ =====
        _safe_print(f"\n[æ¸è¿›å¼VLM] S2é˜¶æ®µï¼šå‡çº§éªŒè¯ ({len(raw_images_escalated)}å¸§)")
        _safe_print(f"[æ¸è¿›å¼VLM] å‡çº§åŸå› : {escalation_reason}")

        result_s2 = self.analyze_progressive(
            raw_images=raw_images_escalated,
            metadata_text=metadata_text_escalated,
            user_query=user_query,
            clip_info=clip_info,
            mode="ESCALATED"
        )

        # è§£æS2ç»“æœ
        result_s2 = parse_vlm_response_with_verdict(result_s2)
        verdict_s2 = result_s2.get("verdict", "NO")
        conf_s2 = result_s2.get("confidence", 0.0)

        # ===== ç»“æœè§£æ =====
        # ä¿å®ˆç­–ç•¥ï¼šä¼˜å…ˆé¿å…FPRä¸Šå‡
        if resolution_conservative:
            # S2=YES â†’ YESï¼ˆä¿¡ä»»S2ï¼‰
            if verdict_s2 == "YES":
                final_verdict = "YES"
                final_conf = conf_s2
                final_has_accident = True
            # S2=POST_EVENT_ONLY ä¸”é«˜ç½®ä¿¡ â†’ POST_EVENT_ONLY
            elif verdict_s2 == "POST_EVENT_ONLY" and conf_s2 >= 0.9:
                final_verdict = "POST_EVENT_ONLY"
                final_conf = conf_s2
                final_has_accident = True
            # S2=UNCERTAIN ä¸”é«˜ç½®ä¿¡ â†’ UNCERTAIN
            elif verdict_s2 == "UNCERTAIN" and conf_s2 >= 0.9:
                final_verdict = "UNCERTAIN"
                final_conf = conf_s2
                final_has_accident = False
            # å…¶ä»–æƒ…å†µä¿æŒS1ç»“æœï¼ˆä¿å®ˆï¼‰
            else:
                final_verdict = verdict_s1
                final_conf = conf_s1
                final_has_accident = result_s1.get("has_accident", False)
        else:
            # éä¿å®ˆç­–ç•¥ï¼šä¿¡ä»»S2
            final_verdict = verdict_s2
            final_conf = conf_s2
            final_has_accident = result_s2.get("has_accident", False)

        _safe_print(f"[æ¸è¿›å¼VLM] è§£æç»“æœ: S1={verdict_s1}({conf_s1:.2f}) â†’ S2={verdict_s2}({conf_s2:.2f}) â†’ Final={final_verdict}")

        # åˆå¹¶accidents
        accidents = result_s2.get("accidents", []) or result_s1.get("accidents", [])

        return {
            "verdict": final_verdict,
            "confidence": final_conf,
            "has_accident": final_has_accident,
            "accidents": accidents,
            "text_summary": result_s2.get("text_summary") or result_s1.get("text_summary", ""),
            "escalated": True,
            "stage1_result": result_s1,
            "stage2_result": result_s2,
            "escalation_reason": escalation_reason,
            "_progressive_vlm": True,
        }

    def _detect_difficult_scene(
        self,
        text_summary: str,
        stage3_config: Optional[dict] = None
    ) -> Tuple[bool, List[str]]:
        """æ£€æµ‹å›°éš¾åœºæ™¯ï¼ˆå¤œé—´/é›¨å¤©/é›¾å¤©/é›ªå¤©ï¼‰

        Args:
            text_summary: VLMè¿”å›çš„æ–‡æœ¬æ‘˜è¦
            stage3_config: Stage3é…ç½®ï¼ˆå¯é€‰ï¼‰

        Returns:
            (is_difficult, scene_conditions) å…ƒç»„
        """
        if stage3_config is None:
            # é»˜è®¤å…³é”®è¯
            night_kw = ["å¤œé—´", "å¤œæ™š", "å¤œè‰²", "é»‘æš—", "ç¯å…‰", "è½¦ç¯", "è·¯ç¯", "å…‰çº¿ä¸è¶³", "low light"]
            rain_kw = ["é›¨å¤©", "é›¨æ°´", "ä¸‹é›¨", "æ¹¿æ»‘", "é›¨å¤œ", "é›¨ä¸­", "rain", "wet"]
            snow_kw = ["é›ªå¤©", "ä¸‹é›ª", "ç§¯é›ª", "é›ªåœ°", "å†°é›ª", "snow"]
            fog_kw = ["é›¾å¤©", "å¤§é›¾", "æµ“é›¾", "èƒ½è§åº¦ä½", "fog", "visibility"]
        else:
            night_kw = stage3_config.get("night_keywords", [])
            rain_kw = stage3_config.get("rain_keywords", [])
            snow_kw = stage3_config.get("snow_keywords", [])
            fog_kw = stage3_config.get("fog_keywords", [])

        text_lower = text_summary.lower()
        conditions = []

        # æ£€æµ‹å„ç±»å›°éš¾åœºæ™¯
        if any(kw in text_summary or kw.lower() in text_lower for kw in night_kw):
            conditions.append("å¤œé—´/ä½å…‰ç…§")
        if any(kw in text_summary or kw.lower() in text_lower for kw in rain_kw):
            conditions.append("é›¨å¤©/æ¹¿æ»‘è·¯é¢")
        if any(kw in text_summary or kw.lower() in text_lower for kw in snow_kw):
            conditions.append("é›ªå¤©/å†°é›ªè·¯é¢")
        if any(kw in text_summary or kw.lower() in text_lower for kw in fog_kw):
            conditions.append("é›¾å¤©/ä½èƒ½è§åº¦")

        return len(conditions) > 0, conditions

    def analyze_progressive_three_stage(
        self,
        raw_images_fast: List[str],
        raw_images_escalated: List[str],
        raw_images_s3: List[str],
        metadata_text_fast: str,
        metadata_text_escalated: str,
        metadata_text_s3: str,
        user_query: str,
        clip_info: Optional[Dict] = None,
        stage3_config: Optional[dict] = None,
        # S1/S2å‚æ•°ï¼ˆç»§æ‰¿è‡ªä¸¤é˜¶æ®µï¼‰
        escalate_on_verdicts: List[str] = None,
        escalate_on_conflict: bool = True,
        conflict_risk_threshold: float = 0.6,
        resolution_conservative: bool = True,
        escalate_on_low_conf_no: bool = True,
        low_conf_no_threshold: float = 0.5,
        escalate_on_high_signal: bool = True,
        high_signal_threshold: float = 0.7,
        max_signal_score: float = 0.0,
    ) -> Dict:
        """ä¸‰é˜¶æ®µæ¸è¿›å¼VLMåˆ†æ - æ”¯æŒS3å›°éš¾åœºæ™¯å¢å¼º

        å½“S2è¿”å›NOä½†æ£€æµ‹åˆ°å›°éš¾åœºæ™¯æ—¶ï¼Œä½¿ç”¨å¢å¼ºpromptè¿›è¡ŒS3åˆ†æã€‚

        Args:
            raw_images_s3: S3é˜¶æ®µå¸§ï¼ˆä¸S2ç›¸åŒæˆ–æ›´å¤šï¼‰
            metadata_text_s3: S3å…ƒæ•°æ®
            stage3_config: Stage3é…ç½®å­—å…¸
            å…¶ä»–å‚æ•°åŒ analyze_progressive_two_stage

        Returns:
            åŒ…å«ä¸‰é˜¶æ®µç»“æœçš„å­—å…¸
        """
        # é¦–å…ˆæ‰§è¡ŒS1+S2
        result_two_stage = self.analyze_progressive_two_stage(
            raw_images_fast=raw_images_fast,
            raw_images_escalated=raw_images_escalated,
            metadata_text_fast=metadata_text_fast,
            metadata_text_escalated=metadata_text_escalated,
            user_query=user_query,
            clip_info=clip_info,
            escalate_on_verdicts=escalate_on_verdicts,
            escalate_on_conflict=escalate_on_conflict,
            conflict_risk_threshold=conflict_risk_threshold,
            resolution_conservative=resolution_conservative,
            escalate_on_low_conf_no=escalate_on_low_conf_no,
            low_conf_no_threshold=low_conf_no_threshold,
            escalate_on_high_signal=escalate_on_high_signal,
            high_signal_threshold=high_signal_threshold,
            max_signal_score=max_signal_score,
        )

        # æ£€æŸ¥æ˜¯å¦éœ€è¦S3
        if stage3_config is None or not stage3_config.get("enabled", False):
            return result_two_stage

        verdict_s2 = result_two_stage.get("verdict", "NO")

        # S3è§¦å‘æ¡ä»¶æ£€æŸ¥
        need_s3 = False
        if stage3_config.get("trigger_on_s2_no", True) and verdict_s2 == "NO":
            need_s3 = True
        if stage3_config.get("trigger_on_s2_uncertain", True) and verdict_s2 == "UNCERTAIN":
            need_s3 = True

        if not need_s3:
            return result_two_stage

        # æ£€æµ‹å›°éš¾åœºæ™¯
        text_summary = result_two_stage.get("text_summary", "")
        s2_result = result_two_stage.get("stage2_result", {})
        if s2_result:
            text_summary = s2_result.get("text_summary", "") or text_summary

        is_difficult, scene_conditions = self._detect_difficult_scene(text_summary, stage3_config)

        if not is_difficult:
            # éå›°éš¾åœºæ™¯ï¼Œä¸è§¦å‘S3
            return result_two_stage

        # ===== S3: å›°éš¾åœºæ™¯å¢å¼ºåˆ†æ =====
        _safe_print(f"\n[æ¸è¿›å¼VLM] S3é˜¶æ®µï¼šå›°éš¾åœºæ™¯å¢å¼ºåˆ†æ ({len(raw_images_s3)}å¸§)")
        _safe_print(f"[æ¸è¿›å¼VLM] æ£€æµ‹åˆ°å›°éš¾åœºæ™¯: {', '.join(scene_conditions)}")

        # æ„å»ºå¢å¼ºprompt
        if stage3_config.get("prompt_injection_enabled", True):
            prompt_prefix = stage3_config.get("difficult_scene_prompt_prefix", "")
            scene_text = ", ".join(scene_conditions)
            enhanced_query = prompt_prefix.replace("{scene_conditions}", scene_text) + "\n" + user_query
        else:
            enhanced_query = user_query

        # æ‰§è¡ŒS3åˆ†æ
        result_s3 = self.analyze_progressive(
            raw_images=raw_images_s3,
            metadata_text=metadata_text_s3,
            user_query=enhanced_query,
            clip_info=clip_info,
            mode="ESCALATED"  # S3ä½¿ç”¨ESCALATEDæ¨¡å¼å¸§æ•°
        )

        # è§£æS3ç»“æœ
        result_s3 = parse_vlm_response_with_verdict(result_s3)
        verdict_s3 = result_s3.get("verdict", "NO")
        conf_s3 = result_s3.get("confidence", 0.0)

        _safe_print(f"[æ¸è¿›å¼VLM] S3ç»“æœ: verdict={verdict_s3}, conf={conf_s3:.2f}")

        # ç¡®å®šæœ€ç»ˆç»“æœ
        # S3ä¼˜å…ˆç­–ç•¥ï¼šS3çš„YES/UNCERTAIN/POST_EVENT_ONLYä¼˜å…ˆäºS2çš„NO
        if verdict_s3 in ["YES", "POST_EVENT_ONLY"]:
            final_verdict = verdict_s3
            final_conf = conf_s3
            final_has_accident = True
        elif verdict_s3 == "UNCERTAIN":
            # å¯é€‰ï¼šå°†é«˜ç½®ä¿¡åº¦UNCERTAINæå‡ä¸ºYES
            if stage3_config.get("boost_uncertain_to_yes", False):
                threshold = stage3_config.get("uncertain_boost_threshold", 0.7)
                if conf_s3 >= threshold:
                    final_verdict = "YES"
                    final_conf = conf_s3
                    final_has_accident = True
                else:
                    final_verdict = "UNCERTAIN"
                    final_conf = conf_s3
                    final_has_accident = False
            else:
                final_verdict = "UNCERTAIN"
                final_conf = conf_s3
                final_has_accident = False
        else:
            # S3ä»ç„¶è¿”å›NOï¼Œä¿æŒS2ç»“æœ
            final_verdict = result_two_stage.get("verdict", "NO")
            final_conf = result_two_stage.get("confidence", 0.0)
            final_has_accident = result_two_stage.get("has_accident", False)

        _safe_print(f"[æ¸è¿›å¼VLM] ä¸‰é˜¶æ®µæœ€ç»ˆ: {verdict_s2} -> S3={verdict_s3} -> Final={final_verdict}")

        # åˆå¹¶ç»“æœ
        accidents = result_s3.get("accidents", []) or result_two_stage.get("accidents", [])

        return {
            "verdict": final_verdict,
            "confidence": final_conf,
            "has_accident": final_has_accident,
            "accidents": accidents,
            "text_summary": result_s3.get("text_summary") or result_two_stage.get("text_summary", ""),
            "escalated": result_two_stage.get("escalated", False),
            "stage3_used": True,
            "stage3_reason": f"difficult_scene: {', '.join(scene_conditions)}",
            "stage1_result": result_two_stage.get("stage1_result"),
            "stage2_result": result_two_stage.get("stage2_result"),
            "stage3_result": result_s3,
            "escalation_reason": result_two_stage.get("escalation_reason", ""),
            "_progressive_vlm": True,
            "_s3_n_frames": len(raw_images_s3),
            "s3_images_count": len(raw_images_s3),
        }
